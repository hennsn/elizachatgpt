{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML News\n",
    "\n",
    "- Chatbot Arena: https://chat.lmsys.org/?arena\n",
    "\n",
    "- LaMini Series: https://github.com/mbzuai-nlp/LaMini-LM\n",
    "    \n",
    "- OpenLLama: https://github.com/openlm-research/open_llama\n",
    "\n",
    "- WizardVicuna: https://github.com/nlpxucan/WizardLM,  https://huggingface.co/junelee/wizard-vicuna-13b\n",
    "\n",
    "- MPT-7 Series: https://www.mosaicml.com/blog/mpt-7b\n",
    "\n",
    "- Open-Assistant Announces Plugins: https://open-assistant.io/chat/06458aa3-d660-763c-8000-fad0bb3cf277\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens und Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abide', 'accelerate', 'accept', 'accomplish', 'achieve', 'acquire', 'acted', 'activate', 'adapt', 'add']\n",
      "num words:  1041\n",
      "len dictinct characters:  26\n",
      "distinct_characters:  ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Num Characters:  27\n",
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n",
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, '_': 26}\n",
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z', 26: '_'}\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATA\n",
    "import torch\n",
    "\n",
    "# read words from file\n",
    "words = open('./english_verbs.txt', 'r').read().splitlines()\n",
    "# alternative dataset: https://github.com/dwyl/english-words\n",
    "#words = open('./words_alpha.txt', 'r').read().splitlines()\n",
    "\n",
    "print(words[:10])\n",
    "\n",
    "print('num words: ', len(words)) \n",
    "\n",
    "# Prepare Alphabet\n",
    "## count characters \n",
    "dataset_characters = []\n",
    "for word in words:\n",
    "    word_characters = list(word)\n",
    "    dataset_characters.extend(word_characters)\n",
    "distinct_characters = sorted(list(set(dataset_characters)))\n",
    "print('len dictinct characters: ', len(distinct_characters))\n",
    "print('distinct_characters: ', distinct_characters)\n",
    "\n",
    "special_characters = ['_'] # changed to blank only for convenience reasons\n",
    "\n",
    "# ngram characters = distinct characters + start token and end token -> + 2\n",
    "num_characters = len(distinct_characters) + len(special_characters)\n",
    "print('Num Characters: ', num_characters)\n",
    "\n",
    "# create a character to index mapping because it is easier to work with indices when using tensor matrices -> every character gets assigned an index\n",
    "character_to_index_map = {character:index for index, character in enumerate(distinct_characters)}\n",
    "print(character_to_index_map)\n",
    "\n",
    "# add our special characters that symbolize start and end of a word\n",
    "character_to_index_map['_'] = 26\n",
    "print(character_to_index_map)\n",
    "\n",
    "# write characters into the cells to make it look more nicely\n",
    "index_to_character_map = {index:character for character, index in character_to_index_map.items()}\n",
    "print(index_to_character_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Objective\n",
    "\n",
    "How do we summarize the quality of our language model? -> Maximum Likelihood Approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3897)\n"
     ]
    }
   ],
   "source": [
    "# Loss Function / Objective Function: \n",
    "\n",
    "## turn the bigram matrix into a probability matrix\n",
    "# create a bigram matrix with rows = characters, columns = follow up characters\n",
    "#bigram_matrix = torch.zeros((num_characters,num_characters), dtype=torch.int32) # we want to represent counts -> use integer, -> there are 26 characters in latin alphabet/roman alphabet LOWER CASED + start/end token, we could also figured that out by a character count ...\n",
    "# model smoothing\n",
    "bigram_matrix = torch.ones((num_characters,num_characters), dtype=torch.int32) # we want to represent counts -> use integer, -> there are 26 characters in latin alphabet/roman alphabet LOWER CASED + start/end token, we could also figured that out by a character count ...\n",
    "#print(bigram_matrix)\n",
    "\n",
    "# BIGRAM MODEL\n",
    "for word in words: \n",
    "    word_characters = ['_'] + list(word) + ['_'] # create a list of all characters we have seen so far\n",
    "    for character_1, character_2, in zip(word_characters, word_characters[1:]): # zip aligns 2 lists => zip/[a,b,c],[d,e,f]) -> [(a,d), (b,e), (c,f)] -> zip(smile, mile) -> [(s,m), (m,i), ...], zip halts once one list is finished\n",
    "        index_1 = character_to_index_map[character_1]\n",
    "        index_2 = character_to_index_map[character_2]\n",
    "        bigram_matrix[index_1, index_2] += 1       \n",
    "#print(bigram_matrix)\n",
    "## compute probs\n",
    "prob_matrix = bigram_matrix.float()\n",
    "prob_matrix /= prob_matrix.sum(1, keepdim=True) # divide by row sum\n",
    "#print(prob_matrix)\n",
    "\n",
    "# COMPUTE QUALITY METRIC ON WHOLE DATASET \n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "for word in words: \n",
    "    word_characters = ['_'] + list(word) + ['_'] # create a list of all characters we have seen so far\n",
    "    for character_1, character_2, in zip(word_characters, word_characters[1:]): # zip aligns 2 lists => zip/[a,b,c],[d,e,f]) -> [(a,d), (b,e), (c,f)] -> zip(smile, mile) -> [(s,m), (m,i), ...], zip halts once one list is finished        \n",
    "        index_1 = character_to_index_map[character_1]\n",
    "        index_2 = character_to_index_map[character_1]\n",
    "        prob = prob_matrix[index_1, index_2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n +=1\n",
    "        #print(f'{character_1}{character_2}: {prob:.4f}')\n",
    "        #print(f'{character_1}{character_2}: {logprob:.4f}')\n",
    "\n",
    "#print(log_likelihood)\n",
    "negative_log_likelihood = -log_likelihood\n",
    "normalized_negative_log_likelihood = negative_log_likelihood/n\n",
    "print(normalized_negative_log_likelihood) # average neg log likelihood\n",
    "\n",
    "# -> summarize the quality of this model in a single number -> avg neg log likelihood loss over the training dataset\n",
    "\n",
    "## -> idea: Likelihood = the product of the probabilities that make up a word\n",
    "## Maximum Likelihood = the word that is most likely\n",
    "## Maximum Log Likelihood -> used for convenience because likelihood gets very low when vocab increases and word size gets big ..\n",
    "\n",
    "### -> just use the log transform on the probability product -> the less negative the number the better = the higher the probability of a word\n",
    "### the idea is then to compute the log likelihood over the whole corpus -> and then take this as the evaluation  of our model = how well performs our model on our corpus which we assume as representative for our language\n",
    "### if the model is perfect -> log likelihoood --> 0, if model is very bad --> neg. infinity\n",
    "\n",
    "### alternative: sometimes negative log likelihood is used because then we want the loss to be good if it is low -> use this as loss function\n",
    "### normalized negative log likelihood -> just normalize the average log likelihood per prediction in the dataset -> this tells us how sure am i on average on each prediction that i make. \n",
    "### -> this average/normalized neg log likelihood over our dataset gives us a quality of our model. the lower the better. \n",
    "\n",
    "### the goal in training is to minimize the avg. neg. log likelihood loss over our dataset \n",
    "### when using a neural network -> we can minimize this loss by updating the parameters of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Models\n",
    "\n",
    "What is our objective? Predict the character in a sequence\n",
    "\n",
    "First naive approach -> compress the look up table that we had before from counting statistics into the weights of a neural network -> let a neural network approximate this discrete function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([26,  0,  1,  ..., 14, 14, 12])\n",
      "tensor([ 0,  1,  8,  ..., 14, 12, 26])\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA\n",
    "# create training set for neural network = all bigrams of our dataset\n",
    "inputs, outputs = [], [] # (x,y)\n",
    "\n",
    "for word in words: \n",
    "    word_characters = ['_'] + list(word) + ['_'] # create a list of all characters we have seen so far\n",
    "    for character_1, character_2, in zip(word_characters, word_characters[1:]): # zip aligns 2 lists => zip/[a,b,c],[d,e,f]) -> [(a,d), (b,e), (c,f)] -> zip(smile, mile) -> [(s,m), (m,i), ...], zip halts once one list is finished        \n",
    "        index_1 = character_to_index_map[character_1]\n",
    "        index_2 = character_to_index_map[character_2]\n",
    "        inputs.append(index_1)\n",
    "        outputs.append(index_2)\n",
    "    \n",
    "\n",
    "# turn training data into tensors\n",
    "inputs = torch.tensor(inputs)\n",
    "outputs = torch.tensor(outputs)\n",
    "print(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "torch.Size([6961, 27])\n",
      "torch.Size([6961, 27])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# ONE HOT ENCODING of CHARACTERS for complete input vector\n",
    "inputs_encoded = F.one_hot(inputs, num_classes=num_characters).float()\n",
    "print(inputs_encoded[0])\n",
    "print(inputs_encoded.shape)\n",
    "outputs_encoded = F.one_hot(outputs, num_classes=num_characters).float()\n",
    "print(outputs_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1548],\n",
      "        [-1.4748],\n",
      "        [ 2.6216],\n",
      "        ...,\n",
      "        [ 1.2950],\n",
      "        [ 1.2950],\n",
      "        [-0.1914]])\n"
     ]
    }
   ],
   "source": [
    "# SINGLE NEURON/PERCEPTRON\n",
    "\n",
    "## -> single neuron neural network -> only 1 weight matrix W -> these weights are multiplied by inputs\n",
    "W = torch.randn((num_characters,1)) # nn.Linear(input_dim, output_dim) > nn.Linear(27, 1)\n",
    "\n",
    "# multiply input with weights -> vector matrix product (x,27)(27,1) -> (x,1) -> activation for every input\n",
    "neuron_output = inputs_encoded @ W\n",
    "print(neuron_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0474, 0.0125, 0.0121,  ..., 0.0871, 0.1330, 0.0406],\n",
      "        [0.0161, 0.0609, 0.0333,  ..., 0.0084, 0.0365, 0.0170],\n",
      "        [0.0059, 0.0305, 0.0247,  ..., 0.0018, 0.0177, 0.1357],\n",
      "        ...,\n",
      "        [0.0255, 0.0250, 0.0102,  ..., 0.0170, 0.0333, 0.0170],\n",
      "        [0.0255, 0.0250, 0.0102,  ..., 0.0170, 0.0333, 0.0170],\n",
      "        [0.0418, 0.0409, 0.0746,  ..., 0.0078, 0.0677, 0.0192]])\n",
      "torch.Size([6961, 27])\n",
      "----------\n",
      "training example 0: _, a \n",
      "input to neural net:  26\n",
      "output from neural net:  tensor([0.0474, 0.0125, 0.0121, 0.0051, 0.0174, 0.0365, 0.0808, 0.0242, 0.0214,\n",
      "        0.0336, 0.0229, 0.0166, 0.0086, 0.0370, 0.0185, 0.0222, 0.0059, 0.0262,\n",
      "        0.0966, 0.0283, 0.0886, 0.0434, 0.0227, 0.0108, 0.0871, 0.1330, 0.0406])\n",
      "label actual next character:  0\n",
      "probability assigned by neural net to correct character:  0.04738028347492218\n",
      "log likelihood:  -3.049549102783203\n",
      "negative log likelihood 3.049549102783203\n",
      "----------\n",
      "training example 1: a, b \n",
      "input to neural net:  0\n",
      "output from neural net:  tensor([0.0161, 0.0609, 0.0333, 0.0070, 0.0215, 0.0971, 0.0371, 0.0055, 0.0318,\n",
      "        0.1465, 0.0065, 0.0734, 0.0293, 0.0545, 0.0106, 0.0375, 0.0213, 0.0359,\n",
      "        0.0058, 0.0750, 0.0158, 0.0365, 0.0338, 0.0455, 0.0084, 0.0365, 0.0170])\n",
      "label actual next character:  1\n",
      "probability assigned by neural net to correct character:  0.060902658849954605\n",
      "log likelihood:  -2.798478364944458\n",
      "negative log likelihood 2.798478364944458\n",
      "----------\n",
      "training example 2: b, i \n",
      "input to neural net:  1\n",
      "output from neural net:  tensor([0.0059, 0.0305, 0.0247, 0.0146, 0.0046, 0.0412, 0.0157, 0.0166, 0.1431,\n",
      "        0.0125, 0.0175, 0.0068, 0.0672, 0.0289, 0.0609, 0.0896, 0.0127, 0.0211,\n",
      "        0.0243, 0.0114, 0.0205, 0.0247, 0.1311, 0.0188, 0.0018, 0.0177, 0.1357])\n",
      "label actual next character:  8\n",
      "probability assigned by neural net to correct character:  0.14306245744228363\n",
      "log likelihood:  -1.9444739818572998\n",
      "negative log likelihood 1.9444739818572998\n",
      "----------\n",
      "training example 3: i, d \n",
      "input to neural net:  8\n",
      "output from neural net:  tensor([0.0049, 0.0199, 0.0137, 0.0150, 0.0430, 0.0425, 0.0173, 0.0110, 0.0115,\n",
      "        0.0989, 0.0154, 0.0970, 0.0388, 0.0341, 0.0095, 0.1088, 0.0531, 0.0541,\n",
      "        0.0208, 0.0070, 0.0976, 0.0167, 0.0042, 0.0138, 0.0374, 0.0281, 0.0860])\n",
      "label actual next character:  3\n",
      "probability assigned by neural net to correct character:  0.015047118999063969\n",
      "log likelihood:  -4.196568965911865\n",
      "negative log likelihood 4.196568965911865\n",
      "----------\n",
      "training example 4: d, e \n",
      "input to neural net:  3\n",
      "output from neural net:  tensor([0.0260, 0.0220, 0.0164, 0.0089, 0.0547, 0.0242, 0.0490, 0.0605, 0.0190,\n",
      "        0.0777, 0.0390, 0.0303, 0.0803, 0.0181, 0.0091, 0.0085, 0.0417, 0.0419,\n",
      "        0.0183, 0.0014, 0.0156, 0.0090, 0.0492, 0.0388, 0.0739, 0.0229, 0.1436])\n",
      "label actual next character:  4\n",
      "probability assigned by neural net to correct character:  0.05465363338589668\n",
      "log likelihood:  -2.9067394733428955\n",
      "negative log likelihood 2.9067394733428955\n",
      "----------\n",
      "average neg log likelihood/loss:  2.9791617393493652\n"
     ]
    }
   ],
   "source": [
    "# SINGLE LAYER\n",
    "\n",
    "## -> single neuron neural network -> only 1 weight matrix W -> these weights are multiplied by inputs\n",
    "W = torch.randn((num_characters, num_characters))\n",
    "\n",
    "# multiply input with weights\n",
    "inputs_encoded @ W #(x,27)(27,27) -> (x,27) # = interpret as distribution over output vocab \n",
    "\n",
    "# interpret outputs as log counts of log likelihood -> if output is interpreted as unnormalized probability distribution -> we just need to normalize it -> softmax (= exponentiate + norm)\n",
    "logits = inputs_encoded @ W # log counts\n",
    "# SOFTMAX: apply softmax to convert output logits to probabilities\n",
    "counts = logits.exp() # this is equivalent to the prob matrix we retrieved statistically from counting in ngram models\n",
    "probs = counts / counts.sum(1, keepdim=True) # normalize rows of the output \n",
    "print(probs)\n",
    "print(probs.shape) \n",
    "\n",
    "# we interpret the output per row as the probability distribution over our vocabulary/character vocab\n",
    "## next step = update W matrix/weight matrix such that loss objective gets minimal\n",
    "\n",
    "# COMPUTE LOSS using this weight matrix\n",
    "num_training_examples = 5 #probs.shape[0]\n",
    "neg_log_likelihoods = torch.zeros(num_training_examples)\n",
    "# step by step backpropagation\n",
    "for i in range(num_training_examples):\n",
    "    # get the i-th bigram\n",
    "    x = inputs[i].item() \n",
    "    y = outputs[i].item()\n",
    "    print('----------')\n",
    "    print(f'training example {i}: {index_to_character_map[x]}, {index_to_character_map[y]} ')\n",
    "    print('input to neural net: ', x)\n",
    "    print('output from neural net: ', probs[i])\n",
    "    print('label actual next character: ', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by neural net to correct character: ', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood: ', logp.item())\n",
    "    nll = -logp\n",
    "    print('negative log likelihood', nll.item())\n",
    "    neg_log_likelihoods[i] = nll\n",
    "    \n",
    "print('----------')\n",
    "print('average neg log likelihood/loss: ', neg_log_likelihoods.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([26,  0,  1,  ..., 14, 14, 12])\n",
      "tensor([ 0,  1,  8,  ..., 14, 12, 26])\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "print(inputs)\n",
    "\n",
    "# outputs\n",
    "print(outputs)\n",
    "\n",
    "# SINGLE LAYER NN\n",
    "\n",
    "## randomly initialize weights\n",
    "W = torch.randn((num_characters, num_characters), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6961, 27])\n",
      "tensor(3.5306, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "# FORWARD PASS\n",
    "inputs_enc = F.one_hot(inputs, num_classes=num_characters).float()\n",
    "# run input through weight layer\n",
    "logits = inputs_enc @ W # predict log counts\n",
    "# apply softmax on outputs = normalize to probs\n",
    "counts = logits.exp() # \n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "print(probs.shape)\n",
    "num_training_examples = probs.shape[0]\n",
    "loss = -probs[torch.arange(num_training_examples), outputs].log().mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BACKWARD PASS\n",
    "\n",
    "#print(W.data)\n",
    "#print(W.shape)\n",
    "#print(W.grad)\n",
    "\n",
    "\n",
    "## initialize gradients -> set gradients to zero\n",
    "W.grad = None \n",
    "\n",
    "## backpropagate the loss\n",
    "loss.backward()\n",
    "\n",
    "## print gradients of every weight\n",
    "#print(W.grad.shape) # every element of W.grad tells us the influence of that weight on the loss function\n",
    "#print(W.grad) # that means if the gradient is positive -> if you increase a weight with positive gradient -> you will increase the loss -> if you decrease it -> we will decrease the loss!\n",
    "\n",
    "# UPDATE WEIGHTS\n",
    "learning_rate = 10\n",
    "W.data += -learning_rate * W.grad\n",
    "# iteratively update the forward pass here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Training Examples:  6961\n",
      "3.670109510421753\n",
      "3.1907551288604736\n",
      "3.0170533657073975\n",
      "2.7699875831604004\n",
      "2.748635768890381\n",
      "2.668529987335205\n",
      "2.7009544372558594\n",
      "2.573179006576538\n",
      "2.616485118865967\n",
      "2.558551549911499\n",
      "2.6176836490631104\n",
      "2.5132970809936523\n",
      "2.571725845336914\n",
      "2.513796806335449\n",
      "2.580388069152832\n",
      "2.4856348037719727\n",
      "2.5498664379119873\n",
      "2.4892945289611816\n",
      "2.5588200092315674\n",
      "2.469641923904419\n",
      "2.536808490753174\n",
      "2.4736220836639404\n",
      "2.5445523262023926\n",
      "2.459153175354004\n",
      "2.528049945831299\n",
      "2.4626822471618652\n",
      "2.534363269805908\n",
      "2.4517016410827637\n",
      "2.5217092037200928\n",
      "2.454589605331421\n",
      "2.5267012119293213\n",
      "2.446089029312134\n",
      "2.516853094100952\n",
      "2.448347568511963\n",
      "2.520723819732666\n",
      "2.4416754245758057\n",
      "2.512979030609131\n",
      "2.4433844089508057\n",
      "2.5159342288970947\n",
      "2.4380931854248047\n",
      "2.509795665740967\n",
      "2.439347982406616\n",
      "2.5120184421539307\n",
      "2.435116767883301\n",
      "2.5071210861206055\n",
      "2.436007261276245\n",
      "2.5087671279907227\n",
      "2.4325993061065674\n",
      "2.5048372745513916\n",
      "2.4332029819488525\n"
     ]
    }
   ],
   "source": [
    "# COMPLETE TRAINING LOOP\n",
    "\n",
    "# CREATE DATA: create training set for neural network = all bigrams of our dataset\n",
    "inputs, outputs = [], [] # (x,y)\n",
    "\n",
    "for word in words: \n",
    "    word_characters = ['_'] + list(word) + ['_'] # create a list of all characters we have seen so far\n",
    "    for character_1, character_2, in zip(word_characters, word_characters[1:]): # zip aligns 2 lists => zip/[a,b,c],[d,e,f]) -> [(a,d), (b,e), (c,f)] -> zip(smile, mile) -> [(s,m), (m,i), ...], zip halts once one list is finished        \n",
    "        index_1 = character_to_index_map[character_1]\n",
    "        index_2 = character_to_index_map[character_2]\n",
    "        inputs.append(index_1)\n",
    "        outputs.append(index_2)\n",
    "    \n",
    "    \n",
    "# turn training data into tensors\n",
    "inputs = torch.tensor(inputs)\n",
    "outputs = torch.tensor(outputs)\n",
    "#print(inputs)\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 100\n",
    "\n",
    "# CREATE NEURAL NET\n",
    "W = torch.randn((num_characters, num_characters), requires_grad=True) # same to torch.Linear(input_dim, output_dim)\n",
    "\n",
    "# RUN TRAINING\n",
    "print('Num Training Examples: ', inputs.shape[0])\n",
    "\n",
    "for k in range(num_epochs):\n",
    "    # forward pass\n",
    "    inputs_enc = F.one_hot(inputs, num_classes=num_characters).float()\n",
    "    logits = inputs_enc @ W\n",
    "    # softmax\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    # compute avg neg log likelihood loss\n",
    "    num_training_examples = probs.shape[0]\n",
    "    loss = -probs[torch.arange(num_training_examples), outputs].log().mean() # + 1*(W**2).mean()\n",
    "    print(loss.item())\n",
    "    # backward pass\n",
    "    W.grad = None # set gradients to zero\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    W.data += -learning_rate * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the best loss we can assume? -> bigram statistics. \n",
    "\n",
    "# Why should we do it using NNs then if we could use our bigram also? -> because this is significantly more scalable and flexible! \n",
    "\n",
    "## For bigrams we get a problem already when we want to compute 3-grams, 4-grams .. -> here we can just add more tokens to the input and predict the same matrix. 27*27*27...\n",
    "## The problem from going away from n-gram models is really that the combinations of follow ups explodes exponentially!\n",
    "\n",
    "## Another advantage of gradient based learning is implicit smoothing via smooth function approximation using regulization! -> counting statistics have to be smoothed for unlikely cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cl_\n",
      "hit_\n",
      "dl_\n",
      "drered_\n",
      "baigeeroudefow_\n"
     ]
    }
   ],
   "source": [
    "# SAMPLING FROM NEURAL NET\n",
    "\n",
    "num_words_to_sample = 5\n",
    "\n",
    "for i in range(num_words_to_sample):\n",
    "    out = []\n",
    "    index = 26 # start with the blank symbol\n",
    "    while True:\n",
    "        x_encoded = F.one_hot(torch.tensor([index]), num_classes=num_characters).float() \n",
    "        logits = x_encoded @ W\n",
    "        # softmax\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        # sampling\n",
    "        index = torch.multinomial(probs, num_samples=1, replacement=True).item()\n",
    "        out.append(index_to_character_map[index])\n",
    "        if index == 26:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron\n",
    "\n",
    "---\n",
    "\n",
    "![](https://www.researchgate.net/publication/354817375/figure/fig2/AS:1071622807097344@1632506195651/Multi-layer-perceptron-MLP-NN-basic-Architecture.jpg)\n",
    "\n",
    "---\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1200/1*EqKiy4-6tuLSoPP_kub33Q.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___  ->  a\n",
      "__a  ->  b\n",
      "_ab  ->  i\n",
      "abi  ->  d\n",
      "bid  ->  e\n",
      "ide  ->  _\n",
      "___  ->  a\n",
      "__a  ->  c\n",
      "_ac  ->  c\n",
      "acc  ->  e\n",
      "cce  ->  l\n",
      "cel  ->  e\n",
      "ele  ->  r\n",
      "ler  ->  a\n",
      "era  ->  t\n",
      "rat  ->  e\n",
      "ate  ->  _\n",
      "___  ->  a\n",
      "__a  ->  c\n",
      "_ac  ->  c\n",
      "acc  ->  e\n",
      "cce  ->  p\n",
      "cep  ->  t\n",
      "ept  ->  _\n",
      "___  ->  a\n",
      "__a  ->  c\n",
      "_ac  ->  c\n",
      "acc  ->  o\n",
      "cco  ->  m\n",
      "com  ->  p\n",
      "omp  ->  l\n",
      "mpl  ->  i\n",
      "pli  ->  s\n",
      "lis  ->  h\n",
      "ish  ->  _\n",
      "tensor([[26, 26, 26],\n",
      "        [26, 26,  0],\n",
      "        [26,  0,  1],\n",
      "        [ 0,  1,  8]])\n",
      "tensor([0, 1, 8, 3])\n"
     ]
    }
   ],
   "source": [
    "# MULTI LAYER PERCEPTRON\n",
    "\n",
    "# Idea: Lets go for bigger contexts! -> 4 gram model -> 3 tokens context predict the upcoming 4th token\n",
    "\n",
    "# Tokens: elements of our fundamental set -> for us this is characters, but could also be words, subwords, ... \n",
    "# [a,b,c,d,_] -> [0,1,2,3,26] embedding -> indices\n",
    "\n",
    "# CREATE DATASET for neural 4 gram model\n",
    "context_length = 3\n",
    "X, Y = [], [] \n",
    "\n",
    "for word in words[:4]: \n",
    "    context = [26] * context_length # _ _ _ word _\n",
    "    for character in word + '_': \n",
    "        index = character_to_index_map[character]\n",
    "        X.append(context)\n",
    "        Y.append(index)\n",
    "        print(''.join(index_to_character_map[i] for i in context), ' -> ', index_to_character_map[index])\n",
    "        context = context[1:] + [index] # crop first context token and append current token as the new last one of the context\n",
    "        \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "print(X[:4])\n",
    "print(Y[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# EMBEDDINGS: Embedding Table -> condensed representation of tokens\n",
    "emb_dim = 2\n",
    "emb_layer = torch.randn((27,emb_dim)) \n",
    "# get emebbing of index 5\n",
    "#print(emb_layer[5])\n",
    "# but how to get it when using one hot encoding? \n",
    "character_embedding = F.one_hot(torch.tensor(5), num_classes=num_characters).float() @ emb_layer\n",
    "#print(character_embedding)\n",
    "# get embeddings of a list or tensor\n",
    "character_embeddings = emb_layer[torch.tensor([5,6,7])]\n",
    "#print(character_embeddings)\n",
    "# or the embeddings of whole tensors\n",
    "x_emb = emb_layer[X]\n",
    "#print(x_emb)\n",
    "print(x_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN LAYER\n",
    "\n",
    "# -> next week: MLPs and bigger context sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "The Pile: https://pile.eleuther.ai/, https://huggingface.co/datasets/EleutherAI/pile\n",
    "\n",
    "The Pile Paper: https://arxiv.org/pdf/2101.00027.pdf, https://arxiv.org/pdf/2201.07311.pdf\n",
    "\n",
    "Llama Paper: https://arxiv.org/abs/2302.13971, https://arxiv.org/pdf/2304.14402.pdf, \n",
    "\n",
    "Red Pajama: https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T, https://www.together.xyz/blog/redpajama, https://github.com/togethercomputer/RedPajama-Data, https://www.together.xyz/blog/redpajama-models-v1\n",
    "\n",
    "Modellgröße + Datenformel: https://arxiv.org/abs/2304.03208, https://arxiv.org/abs/2203.15556, https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/\n",
    "\n",
    "Visual Evaluation: https://github.com/nomic-ai/nomic\n",
    "\n",
    "Quality Estimtimation: Smart Sampling/Statistical Tests + Human Eval, https://arxiv.org/abs/2302.13971, https://s3.amazonaws.com/static.nomic.ai/gpt4all/2023_GPT4All_Technical_Report.pdf\n",
    "\n",
    "## Questions\n",
    "- Wo kommen die Daten her? (etwas philosophischer: Macht die Aufteilung Sinn?)\n",
    "- Wie groß kann ein darauf trainiertes Modell sein? (nach Chinchilla paper oder anderen Empfehlungen)\n",
    "- Welche Schritte wurden im preprocessing durchlaufen? Sind die Daten dadurch tatsächlich \"qualitativ hochwertig\"?\n",
    "- Wie lassen sie sich visualisieren? (Da wird wohl schon ein interaktives Dashboard mittels Meerkat bereitgestellt. Vielleicht damit mal ein bisschen rumprobieren...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
