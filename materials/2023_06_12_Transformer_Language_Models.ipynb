{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML News\n",
    "\n",
    "- Orca LM: -> Not instruction - output pairs are important! -> Instruction -> Reasoning Chain/Explanation -> Output Pairs! = RULES OF LOGIC/SEMANTIC/REASONING\n",
    "    - https://arxiv.org/pdf/2306.02707.pdf\n",
    "    - huggingface is waiting ...: https://huggingface.co/papers/2306.02707\n",
    "- LTM: 5 Milltion Tokens Context\n",
    "    - JFI: Beginning 2022: 1024, Mid: 2023 -> December 2023: GPT-4: 8196 -> April: MPT: 65k -> Juni: LTM: 5M\n",
    "    - https://magic.dev/blog/ltm-1\n",
    "- Bard Logic Update: \n",
    "    - = REPL Tool integrated into Bard -> boost for logic tasks\n",
    "    - https://blog.google/technology/ai/bard-improved-reasoning-google-sheets-export/\n",
    "- SPQR Quantization: = almost LOSLESS 3-4 BIT Quantization is here!\n",
    "    - https://arxiv.org/abs/2306.03078\n",
    "    - possible next step: we have seen QLORA fine tuning -> can we do quantized training from scratch?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115393\n",
      "--------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "--------------\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n",
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "--------------\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "--------------\n",
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# PREPARE DATASET\n",
    "## SHAKESPEARE DATSET\n",
    "\n",
    "with open('./shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "## length of dataset\n",
    "print(\"length of dataset in characters: \", len(text)) \n",
    "print('--------------')\n",
    " \n",
    "## check first 500 characters\n",
    "print(text[:500])\n",
    "print('--------------')\n",
    "\n",
    "## get all characters alphabetically sorted \n",
    "characters = sorted(list(set(text))) \n",
    "vocab_size = len(characters)\n",
    "print(''.join(characters))\n",
    "print(vocab_size)\n",
    "\n",
    "# create a character to index mapping because it is easier to work with indices when using tensor matrices -> every character gets assigned an index\n",
    "character_to_index_map = {character:index for index, character in enumerate(characters)}\n",
    "print(character_to_index_map)\n",
    "print('--------------')\n",
    "# write characters into the cells to make it look more nicely\n",
    "index_to_character_map = {index:character for character, index in character_to_index_map.items()}\n",
    "print(index_to_character_map)\n",
    "\n",
    "# TOKENIZER\n",
    "## encoding/decoding functions to turn strings into lists of integers\n",
    "encode = lambda s: [character_to_index_map[character] for character in s] # encoder takes a string, outputs a list of integers\n",
    "decode = lambda l: ''.join([index_to_character_map[index] for index in l]) # decoder: take a list of integers, outputs a string     \n",
    "\n",
    "print('--------------')\n",
    "\n",
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56])\n",
      "when the input is tensor([18]) then we want to predict: 47\n",
      "when the input is tensor([18, 47]) then we want to predict: 56\n",
      "when the input is tensor([18, 47, 56]) then we want to predict: 57\n",
      "when the input is tensor([18, 47, 56, 57]) then we want to predict: 58\n",
      "when the input is tensor([18, 47, 56, 57, 58]) then we want to predict: 1\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1]) then we want to predict: 15\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1, 15]) then we want to predict: 47\n",
      "when the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) then we want to predict: 58\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[46, 39, 58, 47, 52, 45,  1, 43],\n",
      "        [56,  1, 53, 44,  1, 51, 63,  1],\n",
      "        [57,  1, 46, 43, 39, 56, 58,  6],\n",
      "        [43, 56,  1, 57, 53, 59, 45, 46]])\n",
      "--------------\n",
      "targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[39, 58, 47, 52, 45,  1, 43, 52],\n",
      "        [ 1, 53, 44,  1, 51, 63,  1, 49],\n",
      "        [ 1, 46, 43, 39, 56, 58,  6,  0],\n",
      "        [56,  1, 57, 53, 59, 45, 46, 58]])\n",
      "--------------\n",
      "when the input is [46] then we want to predict: 39\n",
      "when the input is [46, 39] then we want to predict: 58\n",
      "when the input is [46, 39, 58] then we want to predict: 47\n",
      "when the input is [46, 39, 58, 47] then we want to predict: 52\n",
      "when the input is [46, 39, 58, 47, 52] then we want to predict: 45\n",
      "when the input is [46, 39, 58, 47, 52, 45] then we want to predict: 1\n",
      "when the input is [46, 39, 58, 47, 52, 45, 1] then we want to predict: 43\n",
      "when the input is [46, 39, 58, 47, 52, 45, 1, 43] then we want to predict: 52\n",
      "when the input is [56] then we want to predict: 1\n",
      "when the input is [56, 1] then we want to predict: 53\n",
      "when the input is [56, 1, 53] then we want to predict: 44\n",
      "when the input is [56, 1, 53, 44] then we want to predict: 1\n",
      "when the input is [56, 1, 53, 44, 1] then we want to predict: 51\n",
      "when the input is [56, 1, 53, 44, 1, 51] then we want to predict: 63\n",
      "when the input is [56, 1, 53, 44, 1, 51, 63] then we want to predict: 1\n",
      "when the input is [56, 1, 53, 44, 1, 51, 63, 1] then we want to predict: 49\n",
      "when the input is [57] then we want to predict: 1\n",
      "when the input is [57, 1] then we want to predict: 46\n",
      "when the input is [57, 1, 46] then we want to predict: 43\n",
      "when the input is [57, 1, 46, 43] then we want to predict: 39\n",
      "when the input is [57, 1, 46, 43, 39] then we want to predict: 56\n",
      "when the input is [57, 1, 46, 43, 39, 56] then we want to predict: 58\n",
      "when the input is [57, 1, 46, 43, 39, 56, 58] then we want to predict: 6\n",
      "when the input is [57, 1, 46, 43, 39, 56, 58, 6] then we want to predict: 0\n",
      "when the input is [43] then we want to predict: 56\n",
      "when the input is [43, 56] then we want to predict: 1\n",
      "when the input is [43, 56, 1] then we want to predict: 57\n",
      "when the input is [43, 56, 1, 57] then we want to predict: 53\n",
      "when the input is [43, 56, 1, 57, 53] then we want to predict: 59\n",
      "when the input is [43, 56, 1, 57, 53, 59] then we want to predict: 45\n",
      "when the input is [43, 56, 1, 57, 53, 59, 45] then we want to predict: 46\n",
      "when the input is [43, 56, 1, 57, 53, 59, 45, 46] then we want to predict: 58\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING\n",
    "import torch\n",
    " \n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:500]) # the first 500 characters from the print above look to the model like this\n",
    "\n",
    "# TRAIN/VAL SPLIT\n",
    "training_percentage = 0.9\n",
    "n = int(training_percentage * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# set context size\n",
    "context_length = 8 \n",
    "\n",
    "# build training examples for different context lengths starting from 1 to context_length\n",
    "x = train_data[:context_length]\n",
    "y = train_data[1:context_length+1]\n",
    "for t in range(context_length):\n",
    "    context = x[:t+1] # why t+1? -> slicing is exclusive in python\n",
    "    target = y[t]\n",
    "    print(f\"when the input is {context} then we want to predict: {target}\")\n",
    "    \n",
    "# idea here -> do next token prediction but for ALL context lengths up to max_context_length!\n",
    "# why? -> because then we can start generation from single start token\n",
    "\n",
    "# DATA SET COMPILATION\n",
    "batch_size = 4\n",
    "context_length = 8 # = maximum context length\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    indices = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack([data[i:i+context_length] for i in indices])\n",
    "    y = torch.stack([data[i+1:i+context_length+1] for i in indices])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('--------------')\n",
    "print('targets')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('--------------')\n",
    "\n",
    "# test batch loading\n",
    "for b in range(batch_size): # batch dimension \n",
    "    for t in range(context_length): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t] # why t here? -> because we shifted yb already 1 token! -> see above \n",
    "        print(f\"when the input is {context.tolist()} then we want to predict: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "---\n",
    "\n",
    "![](https://heidloff.net/assets/img/2023/02/transformers.png)\n",
    "\n",
    "---\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/9/91/Full_GPT_architecture.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### The Illustrated Transformer\n",
    "- https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMER NETWORK: https://arxiv.org/abs/1706.03762\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Attention: attention is a communication mechanism where a number of nodes in a \n",
    "# connected graph. nodes aggregate information from all the nodes that point to it\n",
    "# attention can be applied to any arbitrary connected graph\n",
    "# attention has no notion of space -> thats why we need a positional encoding (if we want to have a notion of space)\n",
    "# => this is different e.g. from convolution, where aggregation happens over space!\n",
    "\n",
    "# optimizations for deep neural networks: residual connections/skip connections + layer normalization\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_layers):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding = nn.Embedding(context_length, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_heads) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed) # final layer layer norm\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "        token_embeddings = self.embedding_layer(idx)\n",
    "        pos_embeddings = self.position_embedding(torch.arange(T, device=device)) # (T, C)\n",
    "        x = token_embeddings + pos_embeddings\n",
    "        x = self.blocks(x) # apply blocks\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        \n",
    "        if targets is None: \n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last context_length tokens\n",
    "            idx_cond = idx[:, -context_length:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # become (B, C) \n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "            \n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"Implements a single attention head\"\"\" \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False) \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x) # (B, T, C)\n",
    "        q = self.query(x) # B, T, C)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T) -> C = embedding dimension -> divide by embedding dimension to keep the softmax stable, C = n_embed\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B, T, C) # v0*a0 + v1*a1 ....\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"implements multiple heads of self-attention in parallel\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, C)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "        \n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A sinple linear layer followed by a non-linearity\"\"\"\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4* n_embed), # ffwd network\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4* n_embed, n_embed), # projection network\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block. First multi head attention for communication then feed forward network for computation\"\"\"\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.self_attention = MultiHeadAttention(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed) # = per token normalization -> makes the inputs be normal distributed -> stabilizes training\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(self.ln1(x)) # residual connection -> this deviates to original -> we apply layernorm before self attention and before ffwd -> in paper vaswani does it afterwards\n",
    "        x = x + self.ffwd(self.ln2(x)) # residual connection\n",
    "        # layer norm -> normalizes activations to have zero mean and std 1 over all inputs\n",
    "        return x \n",
    "    \n",
    "class LayerNorm():\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        xmean = x.mean(1, keepdim=True) # input mean\n",
    "        xvar = x.var(1, keepdim=True) # input variance\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "        self.out = self.gamma + xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "def estimate_losses(model):\n",
    "    # estimate train loss\n",
    "    xb, yb = get_batch('train')\n",
    "    _, train_loss = model(xb, yb)\n",
    "    # estimate val loss\n",
    "    xb, yb = get_batch('train')\n",
    "    _, val_loss = model(xb, yb)\n",
    "    losses = {'train': train_loss, 'val': val_loss}\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3344, val loss 4.4027\n",
      "step 100: train loss 2.8727, val loss 2.6931\n",
      "step 200: train loss 2.5597, val loss 2.4263\n",
      "step 300: train loss 2.5942, val loss 2.5781\n",
      "step 400: train loss 2.3385, val loss 2.5185\n",
      "step 500: train loss 2.3910, val loss 2.3671\n",
      "step 600: train loss 2.3267, val loss 2.2162\n",
      "step 700: train loss 2.3075, val loss 2.4056\n",
      "step 800: train loss 2.1323, val loss 2.3248\n",
      "step 900: train loss 2.2218, val loss 2.1749\n",
      "step 1000: train loss 2.1252, val loss 2.2314\n",
      "step 1100: train loss 2.2216, val loss 2.0954\n",
      "step 1200: train loss 2.0816, val loss 2.1144\n",
      "step 1300: train loss 2.1495, val loss 2.1939\n",
      "step 1400: train loss 2.1802, val loss 2.1289\n",
      "step 1500: train loss 2.0668, val loss 2.0967\n",
      "step 1600: train loss 2.1368, val loss 2.0730\n",
      "step 1700: train loss 2.1487, val loss 2.0349\n",
      "step 1800: train loss 2.0953, val loss 2.2398\n",
      "step 1900: train loss 1.9716, val loss 2.0479\n",
      "step 1999: train loss 1.9992, val loss 2.0144\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "batch_size = 4\n",
    "context_length = 64\n",
    "max_iters = 2000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embed = 384 # -> 384//6 = 64 -> every attention head is 64 dimensional\n",
    "n_heads = 6 # number of heads in multi head attention\n",
    "n_layers = 2\n",
    "dropout = 0.2 # 20 percent of connections are masked out randomly\n",
    "\n",
    "model = Transformer(n_layers=n_layers)\n",
    "# put model on GPU, if you have one ;)\n",
    "#model = model.to(device)\n",
    "\n",
    "# create optimizer  \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# TRAINING LOOP\n",
    "for iteration in range(max_iters):\n",
    "    # evaluation\n",
    "    if (iteration % eval_interval == 0 or iteration == max_iters-1):\n",
    "        losses = estimate_losses(model)\n",
    "        print(f\"step {iteration}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "     \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GLA:\n",
      "Yold he woaker me ne Wis hirs you,\n",
      "Why that sword he rusd thom bowath' hat'd wicl\n",
      "I Venevry:\n",
      "Frster, are it sas by brain. O!\n",
      "\n",
      "Thy, OF Yhust.\n",
      "\n",
      "KING RIING RIS GICHARGELIET:\n",
      "Havebest bill wam I ging atheige\n",
      "That phe cane theimcy,\n",
      "Son's tir-for them Offech and mainks, ay.\n",
      "\n",
      "Sade snould gragt RIOH:\n",
      "Agve with rence son;\n",
      "Lath be kin, you ispeclor wifl crvest; no pring?\n",
      "\n",
      "Muchtme in therable's not hid cur:\n",
      "Hat If hal be tostle maser.\n",
      "\n",
      "HUMEThe:\n",
      "Comenst shond deexir mand my plagained gon\n",
      "But ondremin t\n"
     ]
    }
   ],
   "source": [
    "# SAMPLING\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device) # this is the new line character \\n -> index 0, batch size 1 -> start generating from the newline character\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
